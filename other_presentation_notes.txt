```
Other Group Projects Notes:
__________________________________

- Nobody is dressed up, casual presentations. Less than business casual.

1.) AI Automation (Job Replacement) Risk
- Prediction
- Kaggle dataset
- Used 4 different models; SVM, Neural Network, Random Forest, KNN
- Used 8-year old data (time sensitive, about AI replacing jobs)
- 30k rows/data entries, considered a lot and enough to be statistically significant

2.) Identifying Emerging Research Domains in AI
- Classification
- Text mining data from arXiV to identify emerging trends
- Processed text into numerical values using BERtopic :scream_cat: 
- Data ranges from 2013-2025
- UMAP (Cosine Similarity) to reduce dimensions from 768 -> 10
- "Very high number of outliers" (20k)... improper data cleaning then? Curious.
- 2024 data used as test set, 2013-2023 used as the training set
- 68% accuracy on logistic regression, 95.89% with Random Forest :)
- Tracked false positive and false negatives (394 false positives)
- W project

3.) Predicting Heart Failure Mortality
- Classification
- UCI 2023 Dataset, artificially generated data, 13 features
- 3 methods - Pearson Correlation, Mutual Information, Random Forest
- StratifiedKFold, GridSearchCV for cross-validations
- Ultimately admitted fault to artificially generated data, what's bad about it, lack of volume etc...
- Model slightly underperforms Seattle Hospital Model


Questions For Other Projects:
_________________________________________

1.) AI Automation (Job Replacement) Risk
- did you create any new data features
- did you change any features from numeric to binary
- among the 4 models you used for your project, which one was the baseline
*- which of the 4 models you used was the "best"
*- how did you ensure that you didnt introduce any noise/errors in your data (from a group that joined two separate datasets)
- when you did correlation analysis did you remove highly correlated or lowest correlated?
- (from previous question)...if you removed any, isnt it possible that either is valuable for the model's performance
- your data is from x time, how could it predict results for today

2.) Identifying Emerging Research Domains in AI
- How do you distinguish between "Emerging" and "Established" categories and what is your dataset?
- How did you calculate that a paper had more citations than another (how did you track citation count)
- Corrected from arXiv, an open-source paper host, did you have any filtering to remove noise or errors
- What if someone maliciously spammed a source like "ChatGPT" in a paper
- What about "spikes" where a conference receives more submissions than another (since it tracks emerging vs. established. This is relevant) and then later dies off?
	- How do you handle these spikes, what's your proposed solution?
- Why use BERtopic and not LDA?
	- Ans: BERtopic performs better, LDA was a good baseline
- Why have a prediction model if you have a dynamic classifier (BERtopic)? (Irrelevant for our project as we want to use both to find out multiple hypotheses)
*- What were the most important features for your prediction?

3.) Predicting Heart Failure Mortality
- Did you manually check if synthetic data matches real data in base stats (mean, med, mode, stddev etc...)
- Is this classification binary or multi-class classification
- Did you have any class imbalance? If so, how did you account for it?
- Did you normalize any data
- Did you try neural networks or just RandomForest
```